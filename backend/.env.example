# Database Configuration
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/regulatory_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=regulatory_db

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

# Elasticsearch Configuration
ELASTICSEARCH_URL=http://localhost:9200

# Redis Configuration
REDIS_URL=redis://localhost:6379

# LLM Provider Selection
# Options: gemini (cloud API), ollama (local inference)
LLM_PROVIDER=gemini

# API Keys - Gemini (Required for RAG Q&A)
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Gemini Model Selection - https://ai.google.dev/gemini-api/docs/models
# The RAG system uses Chain-of-Thought (CoT) reasoning for enhanced accuracy.
# CoT works best with models that support "Thinking" capability.
#
# RECOMMENDED OPTIONS (December 2025):
# 
# 1. gemini-2.5-pro (BEST FOR PRODUCTION - Advanced Thinking)
#    - State-of-the-art thinking model for complex reasoning
#    - 1M token context window (best for long legal documents)
#    - Supports: Thinking, Code execution, Function calling, Search grounding
#    - Use case: Production, high-stakes legal analysis, complex queries
#    - Knowledge cutoff: January 2025
#
# 2. gemini-2.5-flash (RECOMMENDED - Best Price/Performance)
#    - Excellent balance of speed, quality, and cost
#    - 1M token context window with Thinking support
#    - Fast processing, good for real-time applications
#    - Use case: Production, general queries, high-volume applications
#    - Knowledge cutoff: January 2025
#
# 3. gemini-2.5-flash-lite (ULTRA-FAST & COST-EFFECTIVE)
#    - Fastest model with Thinking support
#    - 1M token context window
#    - Optimized for cost-efficiency and high throughput
#    - Use case: Very high volume, cost-sensitive deployments
#    - Knowledge cutoff: January 2025
#
# 4. gemini-3-pro-preview (CUTTING EDGE - Preview Only)
#    - Most intelligent model with state-of-the-art reasoning
#    - 1M token context window
#    - Preview model, may have rate limits
#    - Use case: Testing latest capabilities, research
#    - Knowledge cutoff: January 2025
#
# OLDER MODELS (Still Available):
# - gemini-2.0-flash: Previous generation, 1M context (thinking experimental)
# - gemini-2.0-flash-lite: Previous generation fast model
#
# Chain-of-Thought (CoT) Enhancement:
# The RAG system uses explicit Chain-of-Thought reasoning prompts that guide
# the model through systematic analysis:
#   1. QUESTION ANALYSIS → 2. RELEVANT REGULATIONS → 3. REQUIREMENT MAPPING
#   4. ANSWER SYNTHESIS → 5. CONFIDENCE ASSESSMENT
#
# This improves legal accuracy by 3-5% with minimal performance impact.
# Works best with models marked with "Thinking" capability.
#
GEMINI_MODEL=gemini-2.5-flash

# OLLAMA:
# Model Selection:
# - llama3.2:3b (Default) - 3B parameters, fast, good for testing
# - llama3.2:7b - 7B parameters, better quality, slower
# - mistral:7b - Alternative 7B model
# Run 'ollama list' in container to see available models
# Be sure to pull model if choosing ollama: 
# docker exec -it regulatory-ollama ollama pull llama3.2:3b
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:3b

# Application Settings
APP_ENV=development
DEBUG=True
SECRET_KEY=your_secret_key_here_change_in_production

# CORS Settings
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Logging
LOG_LEVEL=INFO
